{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba919c9-ff72-42ea-867e-fa6013b16810",
   "metadata": {},
   "source": [
    "# Task05. 自定义时序数据集的预处理与插补"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed483ad5-fd01-42c7-a629-f9fcd5db3b89",
   "metadata": {},
   "source": [
    "在本节中，我们将以**合成的 eICU 数据集**为例，演示如何将自定义的医疗时间序列数据预处理为 [PyPOTS](https://github.com/WenjieDu/PyPOTS) 框架所需的输入格式，并使用 PyPOTS 进行插补。\n",
    "\n",
    "## 关于 eICU 数据集\n",
    "\n",
    "> The eICU Collaborative Research Database is a freely available multi-center database for critical care research.  \n",
    "> **Reference**:  \n",
    "> Pollard TJ, Johnson AEW, Raffa JD, Celi LA, Mark RG, and Badawi O. (2018). *The eICU Collaborative Research Database: A multi-center critical care database for research*. Scientific Data. DOI: [10.1038/sdata.2018.178](http://dx.doi.org/10.1038/sdata.2018.178)  \n",
    "> Available at: [https://www.nature.com/articles/sdata2018178](https://www.nature.com/articles/sdata2018178)\n",
    "\n",
    "eICU 数据库包含来自多家医院的 ICU 病患监护记录，是医疗时间序列研究的重要开源资源。在本示例中，我们使用经过脱敏和合成的 eICU 数据集，以避免隐私风险，同时保证数据结构与真实医疗数据一致。\n",
    "\n",
    "## 任务目标\n",
    "\n",
    "- 预处理表格格式的医疗时序数据为 PyPOTS 可用格式。\n",
    "- 使用 PyPOTS 进行插补并还原数据。\n",
    "- 生成可供后续分析或模型训练的数据集。\n",
    "\n",
    "## 主要步骤\n",
    "\n",
    "1. **数据加载**  \n",
    "   加载原始时序数据，包括特征、标签和样本标识。\n",
    "\n",
    "2. **构建三维张量**  \n",
    "   - 将不同样本的特征对齐到统一的时间步长度。\n",
    "   - 构造三维张量 `(n_samples, n_steps, n_features)`。\n",
    "\n",
    "3. **数据插补**  \n",
    "   使用 PyPOTS 提供的插补算法对张量中的缺失值进行填充。\n",
    "\n",
    "4. **还原 DataFrame 结构**  \n",
    "   将插补后的张量转换回 DataFrame 形式，保留样本 ID、时间步、特征和标签。\n",
    "\n",
    "5. **结果保存**  \n",
    "   将插补结果保存为 `.csv` 或 `.npy` 以供后续分析或建模使用。\n",
    "\n",
    "## 结果说明\n",
    "\n",
    "执行完以上步骤后，你将得到三个预处理完成的数据集：\n",
    "- `df_train_imputed`：训练集插补结果\n",
    "- `df_val_imputed`：验证集插补结果\n",
    "- `df_test_imputed`：测试集插补结果\n",
    "\n",
    "## 示例输出检查\n",
    "\n",
    "通过 `.shape` 查看数据集维度，确认处理无误：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f65cc",
   "metadata": {},
   "source": [
    "### 1. 自定义时序数据集的预处理与插补"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174b64a9-e4e9-4dca-b2bb-428b1478476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗\n",
      "╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║\n",
      "   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║\n",
      "   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║\n",
      "   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║\n",
      "   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝\n",
      "ai4ts v0.0.2 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pypots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tsdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from benchpots.utils.logging import logger, print_final_dataset_info\n",
    "from benchpots.utils.missingness import create_missingness # 生成人工缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c99536",
   "metadata": {},
   "source": [
    "### 1.1 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f042d9c-a5da-4ee0-9e5d-966951d5292f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>apacheadmissiondx</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>GCS Total</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Motor</th>\n",
       "      <th>Verbal</th>\n",
       "      <th>admissionheight</th>\n",
       "      <th>...</th>\n",
       "      <th>MAP (mmHg)</th>\n",
       "      <th>Invasive BP Diastolic</th>\n",
       "      <th>Invasive BP Systolic</th>\n",
       "      <th>O2 Saturation</th>\n",
       "      <th>Respiratory Rate</th>\n",
       "      <th>Temperature (C)</th>\n",
       "      <th>glucose</th>\n",
       "      <th>FiO2</th>\n",
       "      <th>pH</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182.9</td>\n",
       "      <td>...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182.9</td>\n",
       "      <td>...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182.9</td>\n",
       "      <td>...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182.9</td>\n",
       "      <td>...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182.9</td>\n",
       "      <td>...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id  timestamp  apacheadmissiondx  ethnicity  gender  GCS Total  \\\n",
       "0          0          0               17.0      394.0   398.0        NaN   \n",
       "1          0          1               17.0      394.0   398.0        NaN   \n",
       "2          0          2               17.0      394.0   398.0      413.0   \n",
       "3          0          3               17.0      394.0   398.0        NaN   \n",
       "4          0          4               17.0      394.0   398.0        NaN   \n",
       "\n",
       "   Eyes  Motor  Verbal  admissionheight  ...  MAP (mmHg)  \\\n",
       "0   NaN    NaN     NaN            182.9  ...        80.0   \n",
       "1   NaN    NaN     NaN            182.9  ...        79.0   \n",
       "2   NaN    NaN     NaN            182.9  ...        75.0   \n",
       "3   NaN    NaN     NaN            182.9  ...        79.0   \n",
       "4   NaN    NaN     NaN            182.9  ...        76.0   \n",
       "\n",
       "   Invasive BP Diastolic  Invasive BP Systolic  O2 Saturation  \\\n",
       "0                   56.0                 119.0           99.0   \n",
       "1                   56.0                 112.0           98.0   \n",
       "2                   56.0                 112.0           98.0   \n",
       "3                   58.0                 108.0           97.0   \n",
       "4                   55.0                 111.0           91.0   \n",
       "\n",
       "   Respiratory Rate  Temperature (C)  glucose  FiO2  pH  label  \n",
       "0               NaN              NaN      NaN   NaN NaN      0  \n",
       "1               NaN              NaN      NaN   NaN NaN      0  \n",
       "2              20.0             35.3      NaN   NaN NaN      0  \n",
       "3               NaN              NaN      NaN   NaN NaN      0  \n",
       "4               NaN              NaN      NaN   NaN NaN      0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('attachments/synthetic_eicu.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f2c9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jv/dw_mq5xs50d8d_vyb8wtdd5m0000gn/T/ipykernel_20377/983142174.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  new_df = df.groupby('sample_id').apply(pad_truncate).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "确保时间步长的一致性：\n",
    "如果自定义数据的时间序列长度不一，则需要通过用缺失值 (NaN) 填充较短的序列或截断较长的序列来对其进行标准化。\n",
    "我们来设置一个最大长度，例如，我们有 48 个时间步长，表示每个患者 48 小时的记录（可以根据数据进行调整）。\n",
    "'''\n",
    "\n",
    "max_length = 48\n",
    "\n",
    "def pad_truncate(df):\n",
    "    if len(df) > max_length:\n",
    "        # 如果 DataFrame 超过最大长度，则截断\n",
    "        # 这里我们选择保留前 max_length 行\n",
    "        # 你也可以选择其他策略，比如保留最后 max_length 行\n",
    "        return df.iloc[:max_length]\n",
    "    else:\n",
    "        # 如果 DataFrame 少于最大长度，则填充\n",
    "        # 这里我们用 NaN 填充\n",
    "        # 你也可以选择其他填充值，比如 0 或者均值等\n",
    "        padding = pd.DataFrame(\n",
    "            index=range(max_length - len(df)),\n",
    "            columns=df.columns\n",
    "        )\n",
    "        if not padding.empty:\n",
    "            return pd.concat([df, padding])\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "# 对每个患者的时间序列进行填充或截断\n",
    "# 这里假设 'sample_id' 是患者的唯一标识符\n",
    "# 你需要根据你的数据集中的实际列名进行调整\n",
    "new_df = df.groupby('sample_id').apply(pad_truncate).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb71c3",
   "metadata": {},
   "source": [
    "### 1.2 数据拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f9c620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame shape: (235584, 23)\n",
      "Validation DataFrame shape: (29472, 23)\n",
      "Test DataFrame shape: (29472, 23)\n",
      "Train features shape: (4908, 48, 20), Train labels shape: (4908,)\n",
      "Validation features shape: (614, 48, 20), Validation labels shape: (614,)\n",
      "Test features shape: (614, 48, 20), Test labels shape: (614,)\n"
     ]
    }
   ],
   "source": [
    "unique_sample_ids = new_df['sample_id'].unique()\n",
    "\n",
    "train_ids, temp_ids = train_test_split(unique_sample_ids, test_size=0.2, random_state=42)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df = new_df[new_df['sample_id'].isin(train_ids)]\n",
    "val_df = new_df[new_df['sample_id'].isin(val_ids)]\n",
    "test_df = new_df[new_df['sample_id'].isin(test_ids)]\n",
    "\n",
    "print(f\"Train DataFrame shape: {train_df.shape}\")\n",
    "print(f\"Validation DataFrame shape: {val_df.shape}\")\n",
    "print(f\"Test DataFrame shape: {test_df.shape}\")\n",
    "\n",
    "# 拆分特征和标签\n",
    "def separate_features_labels(df, feature_cols, label_col='label'):\n",
    "    X = df[feature_cols].values.reshape(-1, 48, len(feature_cols))\n",
    "    # 获取唯一的样本 ID\n",
    "    unique_ids = df['sample_id'].unique()\n",
    "    # 获取每个样本 ID 的第一个标签\n",
    "    y = df.groupby('sample_id')[label_col].first().loc[unique_ids].values\n",
    "    return X, y\n",
    "\n",
    "# 选择特征列\n",
    "feature_columns = [col for col in df.columns if col not in ['sample_id', 'label', 'timestamp']]\n",
    "\n",
    "train_X, train_y = separate_features_labels(train_df.copy(), feature_columns)\n",
    "val_X, val_y = separate_features_labels(val_df.copy(), feature_columns)\n",
    "test_X, test_y = separate_features_labels(test_df.copy(), feature_columns)\n",
    "\n",
    "print(f\"Train features shape: {train_X.shape}, Train labels shape: {train_y.shape}\")\n",
    "print(f\"Validation features shape: {val_X.shape}, Validation labels shape: {val_y.shape}\")\n",
    "print(f\"Test features shape: {test_X.shape}, Test labels shape: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17262d25",
   "metadata": {},
   "source": [
    "### 1.3 数据标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb25725",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Flatten the data before scaling and then reshape it into time series samples\n",
    "train_X = scaler.fit_transform(train_X.reshape(-1, train_X.shape[-1])).reshape(train_X.shape)\n",
    "val_X = scaler.transform(val_X.reshape(-1, val_X.shape[-1])).reshape(val_X.shape)\n",
    "test_X = scaler.transform(test_X.reshape(-1, test_X.shape[-1])).reshape(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eec01125",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = {\n",
    "        # general info\n",
    "        \"n_classes\": len(np.unique(train_y)),\n",
    "        \"n_steps\": train_X.shape[-2],\n",
    "        \"n_features\": train_X.shape[-1],\n",
    "        \"scaler\": scaler,\n",
    "        # train set\n",
    "        \"train_X\": train_X,\n",
    "        \"train_y\": train_y.flatten(),\n",
    "        # val set\n",
    "        \"val_X\": val_X,\n",
    "        \"val_y\": val_y.flatten(),\n",
    "        # test set\n",
    "        \"test_X\": test_X,\n",
    "        \"test_y\": test_y.flatten(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf5f7d",
   "metadata": {},
   "source": [
    "### 1.4 创建人工缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8121c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保留原始数据中的ground truth以用于评估\n",
    "train_X_ori = train_X\n",
    "val_X_ori = val_X\n",
    "test_X_ori = test_X\n",
    "\n",
    "rate = 0.3 # 30%缺失率\n",
    "\n",
    "# 在训练集上创建缺失值作为ground truth\n",
    "train_X = create_missingness(train_X, rate, 'point')\n",
    "\n",
    "# 在验证集上创建缺失值作为ground truth\n",
    "val_X = create_missingness(val_X, rate, 'point' )\n",
    "\n",
    "# 在测试集上创建缺失值作为ground truth\n",
    "test_X = create_missingness(test_X, rate, 'point' )\n",
    "\n",
    "\n",
    "processed_dataset[\"train_X\"] = train_X\n",
    "processed_dataset[\"val_X\"] = val_X\n",
    "processed_dataset[\"test_X\"] = test_X\n",
    "\n",
    "processed_dataset['train_X_ori'] = train_X_ori\n",
    "processed_dataset['val_X_ori'] = val_X_ori\n",
    "processed_dataset['test_X_ori'] = test_X_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1850dd80-9152-4869-baba-ce9c6a956d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 23:34:06 [INFO]: Successfully saved to result_saving/processed_synthetic_eicu.pkl\n"
     ]
    }
   ],
   "source": [
    "from pypots.data.saving import pickle_dump\n",
    "\n",
    "pickle_dump(processed_dataset, \"result_saving/processed_synthetic_eicu.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d154412",
   "metadata": {},
   "source": [
    "### 1.5 准备用于插补的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ff474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算掩码来指示X_ori数据中的真实位置，将被用来评估模型性能\n",
    "\n",
    "train_X_indicating_mask = np.isnan(train_X_ori) ^ np.isnan(train_X)\n",
    "val_X_indicating_mask = np.isnan(val_X_ori) ^ np.isnan(val_X)\n",
    "test_X_indicating_mask = np.isnan(test_X_ori) ^ np.isnan(test_X)\n",
    "\n",
    "# 组装训练集\n",
    "dataset_for_training = {\n",
    "    \"X\": processed_dataset['train_X'],\n",
    "    'X_ori': processed_dataset['train_X_ori'],\n",
    "}\n",
    "\n",
    "# 组装验证集\n",
    "dataset_for_validating = {\n",
    "    \"X\": processed_dataset['val_X'],\n",
    "    \"X_ori\": processed_dataset['val_X_ori'],\n",
    "}\n",
    "\n",
    "# 组装测试集\n",
    "dataset_for_testing = {\n",
    "    \"X\": processed_dataset['test_X'],\n",
    "    \"X_ori\": processed_dataset['test_X_ori'],\n",
    "  }\n",
    "\n",
    "test_X_indicating_mask = np.isnan(processed_dataset['test_X_ori']) ^ np.isnan(processed_dataset['test_X'])\n",
    "\n",
    "# 度量函数不接受 NaN 输入，因此用 0 填充 NaN\n",
    "test_X_ori = np.nan_to_num(processed_dataset['test_X_ori'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dccc3e-d043-45c9-8876-2a08dd889817",
   "metadata": {},
   "source": [
    "# 2. 使用SAITS对自定义数据集中的缺失值进行插补"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec5591",
   "metadata": {},
   "source": [
    "### 2.1 插补数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28f29575-7f31-4791-8350-4220d242f768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 23:34:06 [INFO]: Using the given device: cpu\n",
      "2025-05-10 23:34:06 [INFO]: Model files will be saved to result_saving/imputation/saits/20250510_T233406\n",
      "2025-05-10 23:34:06 [INFO]: Tensorboard file will be saved to result_saving/imputation/saits/20250510_T233406/tensorboard\n",
      "2025-05-10 23:34:06 [INFO]: Using customized MAE as the training loss function.\n",
      "2025-05-10 23:34:06 [INFO]: Using customized MSE as the validation metric function.\n",
      "2025-05-10 23:34:06 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 691,248\n",
      "2025-05-10 23:34:18 [INFO]: Epoch 001 - training loss (MAE): 0.7821, validation MSE: 0.2482\n",
      "2025-05-10 23:34:28 [INFO]: Epoch 002 - training loss (MAE): 0.4996, validation MSE: 0.2092\n",
      "2025-05-10 23:34:40 [INFO]: Epoch 003 - training loss (MAE): 0.4473, validation MSE: 0.1983\n",
      "2025-05-10 23:34:52 [INFO]: Epoch 004 - training loss (MAE): 0.4167, validation MSE: 0.1893\n",
      "2025-05-10 23:35:04 [INFO]: Epoch 005 - training loss (MAE): 0.3988, validation MSE: 0.1915\n",
      "2025-05-10 23:35:14 [INFO]: Epoch 006 - training loss (MAE): 0.3841, validation MSE: 0.1822\n",
      "2025-05-10 23:35:26 [INFO]: Epoch 007 - training loss (MAE): 0.3727, validation MSE: 0.1835\n",
      "2025-05-10 23:35:37 [INFO]: Epoch 008 - training loss (MAE): 0.3639, validation MSE: 0.1818\n",
      "2025-05-10 23:35:48 [INFO]: Epoch 009 - training loss (MAE): 0.3566, validation MSE: 0.1777\n",
      "2025-05-10 23:36:01 [INFO]: Epoch 010 - training loss (MAE): 0.3472, validation MSE: 0.1800\n",
      "2025-05-10 23:36:01 [INFO]: Finished training. The best model is from epoch#9.\n",
      "2025-05-10 23:36:01 [INFO]: Saved the model to result_saving/imputation/saits/20250510_T233406/SAITS.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mean absolute error: 0.2214\n"
     ]
    }
   ],
   "source": [
    "from pypots.nn.functional import calc_mae\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "\n",
    "# 设置模型的运行设备为cpu, 如果你有gpu设备可以设置为cuda\n",
    "DEVICE='cpu'\n",
    "\n",
    "# 创建 SAITS 模型\n",
    "# SAITS 模型的参数可以根据需要进行调整\n",
    "saits = SAITS(\n",
    "    n_steps=processed_dataset['n_steps'],\n",
    "    n_features=processed_dataset['n_features'],\n",
    "    n_layers=1,\n",
    "    d_model=256,\n",
    "    d_ffn=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    # 你可以调整参数ORT_weight和MIT_weight的权重值，以使SAITS模型更多地关注于一个任务。通常你可以让它们保持默认值，比如1\n",
    "    ORT_weight=1,\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # 这里为了快速演示我们将epochs设置为10，你可以将其设置为100或更多以获得更好的结果\n",
    "    epochs=10,\n",
    "    # 这里我们设置patience=3，如果连续3个epoch的评估loss没有减少，则提前停止训练。你可以不设置它,则默认为None,禁用早停机制\n",
    "    patience=3,\n",
    "    # 设置优化器。不同于torch.optim。在初始化pypots.optimizer时，你不必指定模型的参数。您也可以不设置它, 它将默认初始化一个lr=0.001的Adam优化器。\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # 这个num_workers参数用于torch.utils.data.Dataloader。它是用于数据加载的子进程的数量。让它默认为0意味着数据加载将在主进程中，即不会有子进程。如果你认为数据加载是模型训练速度的瓶颈，则可以将其增加\n",
    "    num_workers=0,\n",
    "    # 如果不设置device, PyPOTS将自动为你分配最佳设备。这里我们将其设置为“cpu”。你也可以设置为'cuda', ‘cuda:0’或‘cuda:1’，如果你有多个cuda设备，甚至并行['cuda:0', 'cuda:1']\n",
    "    device=DEVICE,\n",
    "    # 设置保存tensorboard和训练模型文件的路径\n",
    "    saving_path=\"result_saving/imputation/saits\",\n",
    "    # 训练完成后只保存最好的模型。你还可以将其设置为“better”，以保存在训练期间每一次在val set上表现得比之前更好的模型\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "# 训练阶段，使用训练集和验证集\n",
    "saits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
    "\n",
    "# 测试阶段，插补缺失值\n",
    "test_set_imputation = saits.impute(dataset_for_testing)\n",
    "\n",
    "# 根据真实值（人为缺失的值）计算平均绝对误差\n",
    "testing_mae = calc_mae(\n",
    "    test_set_imputation,\n",
    "    test_X_ori,\n",
    "    test_X_indicating_mask,\n",
    ")\n",
    "print(f\"Testing mean absolute error: {testing_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcde375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插补训练集和验证集\n",
    "train_set_imputation = saits.impute(dataset_for_training)\n",
    "val_set_imputation = saits.impute(dataset_for_validating)\n",
    "test_set_imputation = saits.impute(dataset_for_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "983a929e-ab49-490d-ba26-325ffc33edbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 23:36:06 [INFO]: Successfully saved to result_saving/imputed_synthetic_eicu.pkl\n"
     ]
    }
   ],
   "source": [
    "from pypots.data.saving import pickle_dump\n",
    "\n",
    "processed_dataset['train_X'] = train_set_imputation\n",
    "processed_dataset['val_X'] = val_set_imputation\n",
    "processed_dataset['test_X'] = test_set_imputation\n",
    "pickle_dump(processed_dataset, \"result_saving/imputed_synthetic_eicu.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0bc04",
   "metadata": {},
   "source": [
    "### 2.2 如果需要的话可以将3D NumPy数组还原回原始的DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10d4292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataframe(X, labels, sample_ids, scaler, invers_norm = False, n_steps=48):\n",
    "    \"\"\"\n",
    "    Convert 3D NumPy array to a DataFrame with sample_id, timestamp, and original scale features.\n",
    "\n",
    "    Parameters:\n",
    "    - X: 3D NumPy array of shape (n_samples, n_steps, n_features)\n",
    "    - labels: 1D NumPy array of shape (n_samples,) -> labels for each sample\n",
    "    - sample_ids: 1D NumPy array with sample IDs corresponding to each sample\n",
    "    - scaler: Scaler used for normalization (MinMaxScaler/StandardScaler)\n",
    "    - n_steps: Number of time steps (default: 48)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with sample_id, timestamp, features, and labels\n",
    "    \"\"\"\n",
    "    n_samples, _, n_features = X.shape\n",
    "\n",
    "    assert len(feature_columns) == n_features, \"Number of features in X does not match feature_columns\"\n",
    "    assert len(labels) == n_samples, \"Number of labels does not match number of samples\"\n",
    "    assert len(sample_ids) == n_samples, \"Number of sample IDs does not match number of samples\"\n",
    "\n",
    "    # extract the last timestep record for each sample_id  to get one row per sample,\n",
    "    # using the final timestep’s data (e.g., the last hour if n_steps=48 represents hourly data)\n",
    "\n",
    "    X_last = X[:, -1, :]  # Shape: (n_samples, n_features)\n",
    "\n",
    "    # Inverse normalization\n",
    "    if invers_norm:\n",
    "      X_original = scaler.inverse_transform(X_last)\n",
    "    else:\n",
    "      X_original = X_last\n",
    "\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(X_original, columns=feature_columns)\n",
    "    df['sample_id'] = sample_ids\n",
    "    df['timestamp'] = n_steps - 1  # Last timestep (e.g., 47 if 0-indexed)\n",
    "    df['label'] = labels\n",
    "\n",
    "    # Reorder columns: sample_id, timestamp, features, label\n",
    "    df = df[['sample_id', 'timestamp'] + feature_columns + ['label']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "394b3f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4908, 23) (614, 23) (614, 23)\n"
     ]
    }
   ],
   "source": [
    "df_train_imputed = convert_to_dataframe(train_set_imputation, train_y, train_ids, scaler)\n",
    "df_val_imputed = convert_to_dataframe(val_set_imputation, val_y, val_ids, scaler)\n",
    "df_test_imputed = convert_to_dataframe(test_set_imputation, test_y, test_ids, scaler)\n",
    "\n",
    "# 检查数据集的形状\n",
    "print(df_train_imputed.shape, df_val_imputed.shape, df_test_imputed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "768cf394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>apacheadmissiondx</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>GCS Total</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Motor</th>\n",
       "      <th>Verbal</th>\n",
       "      <th>admissionheight</th>\n",
       "      <th>...</th>\n",
       "      <th>MAP (mmHg)</th>\n",
       "      <th>Invasive BP Diastolic</th>\n",
       "      <th>Invasive BP Systolic</th>\n",
       "      <th>O2 Saturation</th>\n",
       "      <th>Respiratory Rate</th>\n",
       "      <th>Temperature (C)</th>\n",
       "      <th>glucose</th>\n",
       "      <th>FiO2</th>\n",
       "      <th>pH</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3098</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.676732</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>0.918308</td>\n",
       "      <td>0.514112</td>\n",
       "      <td>0.545170</td>\n",
       "      <td>0.353181</td>\n",
       "      <td>0.649388</td>\n",
       "      <td>1.156228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318641</td>\n",
       "      <td>-0.023323</td>\n",
       "      <td>0.197020</td>\n",
       "      <td>-0.023408</td>\n",
       "      <td>1.652006</td>\n",
       "      <td>-0.371437</td>\n",
       "      <td>-0.154864</td>\n",
       "      <td>-0.288266</td>\n",
       "      <td>0.489719</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4221</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.516926</td>\n",
       "      <td>0.419554</td>\n",
       "      <td>-1.088959</td>\n",
       "      <td>0.365205</td>\n",
       "      <td>0.246216</td>\n",
       "      <td>0.165949</td>\n",
       "      <td>0.347976</td>\n",
       "      <td>-1.677569</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.578688</td>\n",
       "      <td>-0.616842</td>\n",
       "      <td>-0.466705</td>\n",
       "      <td>-0.496805</td>\n",
       "      <td>-0.905164</td>\n",
       "      <td>-0.806545</td>\n",
       "      <td>-0.528615</td>\n",
       "      <td>-0.356959</td>\n",
       "      <td>0.123013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3154</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.490291</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>-1.088959</td>\n",
       "      <td>0.489465</td>\n",
       "      <td>0.447765</td>\n",
       "      <td>0.338536</td>\n",
       "      <td>0.656788</td>\n",
       "      <td>-0.166210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.800381</td>\n",
       "      <td>-1.012521</td>\n",
       "      <td>-0.289712</td>\n",
       "      <td>0.097490</td>\n",
       "      <td>0.852890</td>\n",
       "      <td>0.080816</td>\n",
       "      <td>-0.154911</td>\n",
       "      <td>-0.352732</td>\n",
       "      <td>0.292381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4041</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.730001</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>-1.088959</td>\n",
       "      <td>0.350713</td>\n",
       "      <td>0.534339</td>\n",
       "      <td>0.342310</td>\n",
       "      <td>0.490669</td>\n",
       "      <td>-1.434142</td>\n",
       "      <td>...</td>\n",
       "      <td>1.144788</td>\n",
       "      <td>1.330135</td>\n",
       "      <td>1.602894</td>\n",
       "      <td>0.097490</td>\n",
       "      <td>3.569884</td>\n",
       "      <td>-0.524262</td>\n",
       "      <td>3.389644</td>\n",
       "      <td>-0.296362</td>\n",
       "      <td>0.271751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2664</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.783270</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>-1.088959</td>\n",
       "      <td>0.188676</td>\n",
       "      <td>0.107571</td>\n",
       "      <td>0.027470</td>\n",
       "      <td>0.398906</td>\n",
       "      <td>-1.248206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.702260</td>\n",
       "      <td>-0.682788</td>\n",
       "      <td>-0.484909</td>\n",
       "      <td>-0.793952</td>\n",
       "      <td>1.172537</td>\n",
       "      <td>0.406375</td>\n",
       "      <td>-0.360595</td>\n",
       "      <td>-0.290909</td>\n",
       "      <td>0.234953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id  timestamp  apacheadmissiondx  ethnicity    gender  GCS Total  \\\n",
       "0       3098         47          -0.676732   0.302200  0.918308   0.514112   \n",
       "1       4221         47          -0.516926   0.419554 -1.088959   0.365205   \n",
       "2       3154         47          -0.490291   0.302200 -1.088959   0.489465   \n",
       "3       4041         47          -0.730001   0.302200 -1.088959   0.350713   \n",
       "4       2664         47          -0.783270   0.302200 -1.088959   0.188676   \n",
       "\n",
       "       Eyes     Motor    Verbal  admissionheight  ...  MAP (mmHg)  \\\n",
       "0  0.545170  0.353181  0.649388         1.156228  ...    0.318641   \n",
       "1  0.246216  0.165949  0.347976        -1.677569  ...   -0.578688   \n",
       "2  0.447765  0.338536  0.656788        -0.166210  ...   -0.800381   \n",
       "3  0.534339  0.342310  0.490669        -1.434142  ...    1.144788   \n",
       "4  0.107571  0.027470  0.398906        -1.248206  ...   -0.702260   \n",
       "\n",
       "   Invasive BP Diastolic  Invasive BP Systolic  O2 Saturation  \\\n",
       "0              -0.023323              0.197020      -0.023408   \n",
       "1              -0.616842             -0.466705      -0.496805   \n",
       "2              -1.012521             -0.289712       0.097490   \n",
       "3               1.330135              1.602894       0.097490   \n",
       "4              -0.682788             -0.484909      -0.793952   \n",
       "\n",
       "   Respiratory Rate  Temperature (C)   glucose      FiO2        pH  label  \n",
       "0          1.652006        -0.371437 -0.154864 -0.288266  0.489719      0  \n",
       "1         -0.905164        -0.806545 -0.528615 -0.356959  0.123013      0  \n",
       "2          0.852890         0.080816 -0.154911 -0.352732  0.292381      0  \n",
       "3          3.569884        -0.524262  3.389644 -0.296362  0.271751      1  \n",
       "4          1.172537         0.406375 -0.360595 -0.290909  0.234953      0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc13be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_imputed.to_csv('result_saving/train_imputed.csv', index=False)\n",
    "df_val_imputed.to_csv('result_saving/val_imputed.csv', index=False)\n",
    "df_test_imputed.to_csv('result_saving/test_imputed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7973ca27-02e3-42aa-9931-cb18c653d8b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. 阅读材料\n",
    "\n",
    "### Wang, J., Du, W., Yang, Y., Qian, L., Cao, W., Zhang, K., Wang, W., Liang, Y. & Wen, Q. (2025) [Deep Learning for Multivariate Time Series Imputation: A Survey](https://arxiv.org/abs/2402.04059). IJCAI 2025.\n",
    "#### 推荐原因: 该文回顾并总结了深度学习在时序插补领域的发展, 文章被人工智能顶级会议IJCAI 2025收录, 五位审稿人均给出正面评价. 截止2025年5月Google Scholar上引用50+."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
