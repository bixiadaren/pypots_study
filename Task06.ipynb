{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b42a636-bf13-4625-af2e-a8eb5c6ea571",
   "metadata": {},
   "source": [
    "# Task06. 自定义数据集下游任务的两种分析方法\n",
    "\n",
    "在本节中，我们将基于 **Task05中自定义的数据集** 进行下游任务分析，主要包括：\n",
    "\n",
    "1. 使用LSTM基于插补后的数据集进行分类\n",
    "2. 基于PyPOTS中TimesNet模型对带缺失值的原数据进行端到端学习的分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f93d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. 基于插补数据的基线分类分析\n",
    "\n",
    "### 1.1 加载插补后的数据集\n",
    "\n",
    "首先，从之前保存的插补结果文件中加载训练集、验证集和测试集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f76ca6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗\n",
      "╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║\n",
      "   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║\n",
      "   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║\n",
      "   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║\n",
      "   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝\n",
      "ai4ts v0.0.2 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pypots.data.saving import pickle_load\n",
    "\n",
    "imputed_dataset = pickle_load('result_saving/imputed_synthetic_eicu.pkl')\n",
    "\n",
    "train_X, val_X, test_X = imputed_dataset['train_X'], imputed_dataset['val_X'], imputed_dataset['test_X']\n",
    "train_y, val_y, test_y = imputed_dataset['train_y'], imputed_dataset['val_y'], imputed_dataset['test_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a7c2c",
   "metadata": {},
   "source": [
    "### 1.2 基于LSTM进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c0715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "\n",
    "# 设置模型的运行设备为cpu, 如果你有gpu设备可以设置为cuda\n",
    "DEVICE='cpu'\n",
    "\n",
    "class LoadImputedDataAndLabel(Dataset):\n",
    "    def __init__(self, imputed_data, labels):\n",
    "        self.imputed_data = imputed_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.imputed_data[idx]).to(torch.float32),\n",
    "            torch.tensor(self.labels[idx]).to(torch.long),\n",
    "        )\n",
    "\n",
    "\n",
    "class ClassificationLSTM(torch.nn.Module):\n",
    "    def __init__(self, n_features, rnn_hidden_size, n_classes):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            n_features,\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fcn = torch.nn.Linear(rnn_hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        hidden_states, _ = self.rnn(data)\n",
    "        logits = self.fcn(hidden_states[:, -1, :])\n",
    "        prediction_probabilities = torch.sigmoid(logits)\n",
    "        return prediction_probabilities\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, test_loader):\n",
    "    n_epochs = 20\n",
    "    patience = 5\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "    current_patience = patience\n",
    "    best_loss = float(\"inf\")\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for idx, data in enumerate(train_dataloader):\n",
    "            X, y = map(lambda x: x.to(DEVICE), data)\n",
    "            optimizer.zero_grad()\n",
    "            probabilities = model(X)\n",
    "            loss = F.cross_entropy(probabilities, y.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        loss_collector = []\n",
    "        with torch.no_grad():\n",
    "            for idx, data in enumerate(val_dataloader):\n",
    "                X, y = map(lambda x: x.to(DEVICE), data)\n",
    "                probabilities = model(X)\n",
    "                loss = F.cross_entropy(probabilities, y.reshape(-1))\n",
    "                loss_collector.append(loss.item())\n",
    "\n",
    "        loss = np.asarray(loss_collector).mean()\n",
    "        if best_loss > loss:\n",
    "            current_patience = patience\n",
    "            best_loss = loss\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "        else:\n",
    "            current_patience -= 1\n",
    "\n",
    "        if current_patience == 0:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    model.eval()\n",
    "\n",
    "    probability_collector = []\n",
    "    for idx, data in enumerate(test_loader):\n",
    "        X, y = map(lambda x: x.to(DEVICE), data)\n",
    "        probabilities = model.forward(X)\n",
    "        probability_collector += probabilities.cpu().tolist()\n",
    "\n",
    "    probability_collector = np.asarray(probability_collector)\n",
    "    return probability_collector\n",
    "\n",
    "\n",
    "def get_dataloaders(train_X, train_y, val_X, val_y, test_X, test_y, batch_size=128):\n",
    "    train_set = LoadImputedDataAndLabel(train_X, train_y)\n",
    "    val_set = LoadImputedDataAndLabel(val_X, val_y)\n",
    "    test_set = LoadImputedDataAndLabel(test_X, test_y)\n",
    "    train_loader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50041793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换成torch dataloader\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    train_X, \n",
    "    train_y, \n",
    "    val_X, \n",
    "    val_y, \n",
    "    test_X, \n",
    "    test_y,\n",
    ")\n",
    "\n",
    "rnn_classifier = ClassificationLSTM(\n",
    "    n_features=imputed_dataset['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "    n_classes=2, # physionet2012是一个二分类数据集\n",
    ")\n",
    "proba_predictions = train(rnn_classifier, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b727e063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_set中的正负样本比例为62:552, 正样本占样本数量的0.10097719869706841, 所以这是一个不平衡的二分类问题, 故我们在此使用ROC-AUC和PR-AUC作为评价指标\n",
      "\n",
      "LSTM在测试集上的ROC-AUC为: 0.6264\n",
      "\n",
      "LSTM在测试集上的PR-AUC为: 0.2690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pypots.nn.functional.classification import calc_binary_classification_metrics\n",
    "\n",
    "pos_num = test_y.sum()\n",
    "neg_num = len(test_y) - test_y.sum()\n",
    "print(f'test_set中的正负样本比例为{pos_num}:{neg_num}, 正样本占样本数量的{pos_num/len(test_y)}, 所以这是一个不平衡的二分类问题, 故我们在此使用ROC-AUC和PR-AUC作为评价指标\\n')\n",
    "\n",
    "classification_metrics=calc_binary_classification_metrics(\n",
    "    proba_predictions, test_y\n",
    ")\n",
    "print(f\"LSTM在测试集上的ROC-AUC为: {classification_metrics['roc_auc']:.4f}\\n\")\n",
    "print(f\"LSTM在测试集上的PR-AUC为: {classification_metrics['pr_auc']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4f723-a1fd-4335-bf78-e31ee9424fb5",
   "metadata": {},
   "source": [
    "## 2. 使用PyPOTS中的TimesNet模型对自定义数据集进行基于端到端的时序建模与分类分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dfb11c",
   "metadata": {},
   "source": [
    "### 2.1 数据集准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee8b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tsdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 设置模型的运行设备为cpu, 如果你有gpu设备可以设置为cuda\n",
    "DEVICE='cpu'\n",
    "\n",
    "from pypots.data.saving import pickle_load\n",
    "processed_dataset = pickle_load('result_saving/processed_synthetic_eicu.pkl')\n",
    "\n",
    "# 组装训练集\n",
    "dataset_for_training = {\n",
    "    \"X\": processed_dataset['train_X'],\n",
    "    'y': processed_dataset['train_y'],\n",
    "}\n",
    "\n",
    "# 组装验证集\n",
    "dataset_for_validating = {\n",
    "    \"X\": processed_dataset['val_X'],\n",
    "    \"y\": processed_dataset['val_y'],\n",
    "}\n",
    "\n",
    "# 组装测试集\n",
    "dataset_for_testing = {\n",
    "    \"X\": processed_dataset['test_X'],\n",
    "    \"y\": processed_dataset['test_y'],\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef314f4",
   "metadata": {},
   "source": [
    "### 2.2 TimesNet建模分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3decbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 00:02:18 [INFO]: Using the given device: cpu\n",
      "2025-05-11 00:02:18 [INFO]: Model files will be saved to result_saving/classification/timesnet/20250511_T000218\n",
      "2025-05-11 00:02:18 [INFO]: Tensorboard file will be saved to result_saving/classification/timesnet/20250511_T000218/tensorboard\n",
      "2025-05-11 00:02:18 [INFO]: Using customized CrossEntropy as the training loss function.\n",
      "2025-05-11 00:02:18 [INFO]: Using customized CrossEntropy as the validation metric function.\n",
      "2025-05-11 00:02:18 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,158,274\n",
      "2025-05-11 00:03:02 [INFO]: Epoch 001 - training loss (CrossEntropy): 0.3349, validation CrossEntropy: 0.2572\n",
      "2025-05-11 00:03:47 [INFO]: Epoch 002 - training loss (CrossEntropy): 0.2741, validation CrossEntropy: 0.2534\n",
      "2025-05-11 00:04:40 [INFO]: Epoch 003 - training loss (CrossEntropy): 0.2626, validation CrossEntropy: 0.2721\n",
      "2025-05-11 00:05:26 [INFO]: Epoch 004 - training loss (CrossEntropy): 0.2372, validation CrossEntropy: 0.2560\n",
      "2025-05-11 00:06:15 [INFO]: Epoch 005 - training loss (CrossEntropy): 0.2101, validation CrossEntropy: 0.2864\n",
      "2025-05-11 00:07:01 [INFO]: Epoch 006 - training loss (CrossEntropy): 0.1952, validation CrossEntropy: 0.2846\n",
      "2025-05-11 00:07:46 [INFO]: Epoch 007 - training loss (CrossEntropy): 0.1736, validation CrossEntropy: 0.2843\n",
      "2025-05-11 00:07:46 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-11 00:07:46 [INFO]: Finished training. The best model is from epoch#2.\n",
      "2025-05-11 00:07:46 [INFO]: Saved the model to result_saving/classification/timesnet/20250511_T000218/TimesNet.pypots\n"
     ]
    }
   ],
   "source": [
    "from pypots.nn.functional import calc_mae\n",
    "from pypots.optim import Adam\n",
    "from pypots.classification import TimesNet\n",
    "\n",
    "# 创建 TimesNet 模型\n",
    "# TimesNet 模型的参数可以根据需要进行调整\n",
    "timesnet = TimesNet(\n",
    "    n_steps=processed_dataset['n_steps'],\n",
    "    n_features=processed_dataset['n_features'],\n",
    "    n_classes=processed_dataset['n_classes'],\n",
    "    n_layers=2,\n",
    "    top_k=3,\n",
    "    d_model=64,\n",
    "    d_ffn=128,\n",
    "    n_kernels=3,\n",
    "    dropout=0.3,\n",
    "    batch_size=32,\n",
    "    # 这里为了快速演示我们将epochs设置为20，你可以将其设置为100或更多以获得更好的结果\n",
    "    epochs=20,\n",
    "    # 这里我们设置patience=5，如果连续5个epoch的评估loss没有减少，则提前停止训练。你可以不设置它,则默认为None,禁用早停机制\n",
    "    patience=5,\n",
    "    # 设置优化器。不同于torch.optim。在初始化pypots.optimizer时，你不必指定模型的参数。您也可以不设置它, 它将默认初始化一个lr=0.001的Adam优化器。\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # 这个num_workers参数用于torch.utils.data.Dataloader。它是用于数据加载的子进程的数量。让它默认为0意味着数据加载将在主进程中，即不会有子进程。如果你认为数据加载是模型训练速度的瓶颈，则可以将其增加\n",
    "    num_workers=0,\n",
    "    # 如果不设置device, PyPOTS将自动为你分配最佳设备。这里我们将其设置为“cpu”。你也可以设置为'cuda', ‘cuda:0’或‘cuda:1’，如果你有多个cuda设备，甚至并行['cuda:0', 'cuda:1']\n",
    "    device=DEVICE,\n",
    "    # 设置保存tensorboard和训练模型文件的路径\n",
    "    saving_path=\"result_saving/classification/timesnet\",\n",
    "    # 训练完成后只保存最好的模型。你还可以将其设置为“better”，以保存在训练期间每一次在val set上表现得比之前更好的模型\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "# 训练阶段，使用训练集和验证集\n",
    "timesnet.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "094ed9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimesNet在测试集上的ROC-AUC为: 0.5743\n",
      "\n",
      "TimesNet在测试集上的PR-AUC为: 0.4171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pypots.nn.functional.classification import calc_binary_classification_metrics\n",
    "\n",
    "timesnet_results = timesnet.predict(dataset_for_testing)\n",
    "timesnet_prediction = timesnet_results[\"classification\"]\n",
    "\n",
    "classification_metrics=calc_binary_classification_metrics(\n",
    "    timesnet_prediction, dataset_for_testing[\"y\"]\n",
    ")\n",
    "print(f\"TimesNet在测试集上的ROC-AUC为: {classification_metrics['roc_auc']:.4f}\\n\")\n",
    "print(f\"TimesNet在测试集上的PR-AUC为: {classification_metrics['pr_auc']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2259c157-53c2-4535-98c9-b58f5a44193e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. 阅读材料\n",
    "\n",
    "### Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., & Long, M. (2023). [TimesNet: Temporal 2d-variation modeling for general time series analysis](https://openreview.net/forum?id=ju_Uqw384Oq). *ICLR 2023*\n",
    "\n",
    "#### 推荐原因: 该文提出了适用于多个时序分析任务的模型结构TimesNet. 文章被人工智能顶级会议ICLR 2023收录. 截止2025年5月Google Scholar上引用1300+."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
